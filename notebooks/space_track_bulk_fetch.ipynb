{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "from typing import Any, Dict, Optional\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import spacetrack.operators as op\n",
    "from spacetrack import SpaceTrackClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_jsons(directory_path):\n",
    "\n",
    "    json_files = glob.glob(os.path.join(directory_path, '*.json'))\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for file in tqdm(json_files, desc=\"Loading JSON files\"):\n",
    "        try:\n",
    "            df = pd.read_json(file)\n",
    "            df['source_file'] = os.path.basename(file)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"\\nLoaded {len(json_files)} files into DataFrame with shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        raise ValueError(\"No JSON files were successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['datetime'] = pd.to_datetime(df['epoch'])\n",
    "#df['year'] = df['datetime'].dt.year\n",
    "#df['month'] = df['datetime'].dt.month\n",
    "\n",
    "#df.to_parquet(r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\udl_CIS_data.parquet\",engine='pyarrow', compression = 'gzip', index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmonthly_counts = df.groupby(['year', 'month']).size().unstack(fill_value=0)\\n\\n# Rename columns to month names for clarity\\nmonth_names = {\\n    1: 'January', 2: 'February', 3: 'March', 4: 'April',\\n    5: 'May', 6: 'June', 7: 'July', 8: 'August',\\n    9: 'September', 10: 'October', 11: 'November', 12: 'December'\\n}\\nmonthly_counts = monthly_counts.rename(columns=month_names)\\n\\n# Add row totals\\nmonthly_counts['Total'] = monthly_counts.sum(axis=1)\\n\\n# Add column totals\\nmonthly_counts.loc['Total'] = monthly_counts.sum()\\n\\nmonthly_counts\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "monthly_counts = df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns to month names for clarity\n",
    "month_names = {\n",
    "    1: 'January', 2: 'February', 3: 'March', 4: 'April',\n",
    "    5: 'May', 6: 'June', 7: 'July', 8: 'August',\n",
    "    9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "}\n",
    "monthly_counts = monthly_counts.rename(columns=month_names)\n",
    "\n",
    "# Add row totals\n",
    "monthly_counts['Total'] = monthly_counts.sum(axis=1)\n",
    "\n",
    "# Add column totals\n",
    "monthly_counts.loc['Total'] = monthly_counts.sum()\n",
    "\n",
    "monthly_counts\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = r\"D:\\Russat\\pull\\udl_CIS_data.parquet\"\n",
    "df = pd.read_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9814048, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['year'] == 2022) & ((df['month']==2) | (df['month']==3) | (df['month']==4))]\n",
    "sat_lst = list(filtered_df['satNo'])\n",
    "norad_id_date_filter_lst = list(set(sat_lst))\n",
    "norad_id_date_filter_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\CIS_satcat.pkl', 'rb') as f:  # 'rb' means read binary mode\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norad_list = [item['NORAD_CAT_ID'] for item in data \n",
    "             if item['OBJECT_TYPE'] in ['ROCKET BODY', 'PAYLOAD', 'UNKNOWN']]\n",
    "norad_id_typefilter_lst = list(set(norad_list))\n",
    "norad_id_typefilter_lst.sort()\n",
    "norad_id_typefilter_lst = [int(x) for x in norad_id_typefilter_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nord_id_typeDate_filter = list(set(norad_id_date_filter_lst) & set(norad_id_typefilter_lst))\n",
    "nord_id_typeDate_filter.sort()\n",
    "len(nord_id_typeDate_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open(r'C:\\Users\\dk412\\Desktop\\spacetrackcreds.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "st_un = content.split(\",\")[0].strip()\n",
    "st_pw = content.split(\",\")[1].strip()\n",
    "udl_un = content.split(\",\")[2].strip()\n",
    "udl_pw = content.split(\",\")[3].strip()\n",
    "\n",
    "st = SpaceTrackClient(identity=f'{st_un}', password=f'{st_pw}')\n",
    "\n",
    "full_lst = []\n",
    "try:\n",
    "    norad_ids = ','.join(str(i) for i in nord_id_typeDate_filter)\n",
    "    query = st.tle(norad_cat_id=norad_ids, orderby='epoch', limit=None, format='tle')\n",
    "    tles = query.split('\\n')\n",
    "    full_lst.extend([tles[i:i+2] for i in range(0, len(tles), 2)])\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    time.sleep(900)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "norad_ids = nord_id_typeDate_filter\n",
    "batch_size = 100\n",
    "all_tles = []\n",
    "total_batches = (len(nord_id_typeDate_filter) + 100 - 1) // 100\n",
    "\n",
    "for i in range(0, len(norad_ids), batch_size):\n",
    "    batch = norad_ids[i:i + batch_size]\n",
    "    batch_str = ','.join(str(id) for id in batch)\n",
    "    batch_num = (i // batch_size) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tle_batch(st_client, norad_ids, batch_size=50):\n",
    "    all_tles = []\n",
    "    total_batches = (len(norad_ids) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {len(norad_ids)} NORAD IDs in {total_batches} batches\")\n",
    "    \n",
    "    for i in range(0, len(norad_ids), batch_size):\n",
    "        batch = norad_ids[i:i + batch_size]\n",
    "        batch_str = ','.join(str(id) for id in batch)\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        print(f\"Fetching batch {batch_num}/{total_batches} ({len(batch)} IDs)\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 4\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                #query = st_client.tle(norad_cat_id=batch_str, orderby='epoch', limit=None, format='tle')\n",
    "                query = st_client.tle(norad_cat_id=batch_str, epoch = '>2022-02-01', orderby='epoch', limit=None, format='tle')\n",
    "                tles = query.split('\\n')\n",
    "                batch_tles = [tles[i:i+2] for i in range(0, len(tles), 2)]\n",
    "                all_tles.extend(batch_tles)\n",
    "                \n",
    "                print(f\"Successfully fetched {len(batch_tles)} TLE pairs\")\n",
    "                time.sleep(5)  \n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Error on batch {batch_num}: {str(e)}\")\n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = 900  \n",
    "                    print(f\"Retrying in {wait_time} seconds... (Attempt {retry_count + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch batch {batch_num} after {max_retries} attempts\")\n",
    "                    \n",
    "    return all_tles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\spacetrackcreds.txt\", 'r') as f:\n",
    "    content = f.read()\n",
    "st_un = content.split(\",\")[0].strip()\n",
    "st_pw = content.split(\",\")[1].strip()\n",
    "udl_un = content.split(\",\")[2].strip()\n",
    "udl_pw = content.split(\",\")[3].strip()\n",
    "\n",
    "st = SpaceTrackClient(identity=f'{st_un}', password=f'{st_pw}')\n",
    "\n",
    "full_lst = fetch_tle_batch(st, nord_id_typeDate_filter, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_scientific_notation(string):\n",
    "    try:\n",
    "        if string.strip() == '+00000-0' or string.strip() == '+00000+0':\n",
    "            return 0.0\n",
    "        \n",
    "        mantissa = float(string[0] + '.' + string[1:6])\n",
    "        exponent = int(string[6:8])\n",
    "        return mantissa * (10 ** exponent)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def parse_tle_to_df(tle_list):\n",
    "    data = []\n",
    "    \n",
    "    for tle in tle_list:\n",
    "        # Skip if not a proper TLE pair\n",
    "        if not isinstance(tle, list) or len(tle) != 2:\n",
    "            print(f\"Skipping invalid TLE pair: {tle}\")\n",
    "            continue\n",
    "            \n",
    "        line1, line2 = tle\n",
    "        line1_data = {\n",
    "            'line1': line1,\n",
    "            'line2': line2,\n",
    "            # Line 1 elements\n",
    "            'catalog_number': int(line1[2:7]),\n",
    "            'classification': line1[7],\n",
    "            'launch_year': line1[9:11],\n",
    "            'launch_number': line1[11:14],\n",
    "            'launch_piece': line1[14:17].strip(),\n",
    "            'epoch_year': int(line1[18:20]),\n",
    "            'epoch_day': float(line1[20:32]),\n",
    "            'mean_motion_dot': float(line1[33:43]),\n",
    "            'mean_motion_ddot': parse_scientific_notation(line1[44:52] + line1[52:54]),\n",
    "            'bstar': parse_scientific_notation(line1[53:61] + line1[61:63]),\n",
    "            'ephemeris_type': int(line1[63]) if line1[63].strip() else 0,\n",
    "            'element_number': int(line1[64:68]) if line1[64:68].strip() else 0,\n",
    "            # Line 2 elements\n",
    "            'satellite_number': int(line2[2:7]),\n",
    "            'inclination': float(line2[8:16]),\n",
    "            'ra_of_asc_node': float(line2[17:25]),\n",
    "            'eccentricity': float('0.' + line2[26:33]),\n",
    "            'arg_of_perigee': float(line2[34:42]),\n",
    "            'mean_anomaly': float(line2[43:51]),\n",
    "            'mean_motion': float(line2[52:63]),\n",
    "            'rev_at_epoch': int(line2[63:68]) if line2[63:68].strip() else 0\n",
    "        }\n",
    "        data.append(line1_data)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = parse_tle_to_df(full_lst)\n",
    "\n",
    "if not df.empty:\n",
    "    df.to_parquet(r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\spacetrack_tle_df.parquet\",engine='pyarrow', compression = 'gzip', index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "#                     PROD CODE- Space Track TLE Fetch\n",
    "####################################################################\n",
    "\n",
    "# Imports\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "from typing import Any, Dict, Optional\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import spacetrack.operators as op\n",
    "from spacetrack import SpaceTrackClient\n",
    "\n",
    "  \n",
    "#DIRECTORIES\n",
    "#Local\n",
    "print('LOCAL')\n",
    "parquet_file = r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\udl_CIS_data.parquet\"\n",
    "sat_cat_pkl = r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\CIS_satcat.pkl\"\n",
    "login_creds = r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\spacetrackcreds.txt\"\n",
    "parquet_out = r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\spacetrack_tle_df.parquet\"\n",
    "\"\"\"\n",
    "\n",
    "#Beocat\n",
    "print('BEOCAT')\n",
    "parquet_file = \"/homes/dkurtenb/projects/russat/output/udl_CIS_data.parquet\"\n",
    "sat_cat_pkl = '/homes/dkurtenb/projects/russat/output/CIS_satcat.pkl'\n",
    "login_creds = '/homes/dkurtenb/projects/russat/spacetrackcreds.txt'\n",
    "parquet_out = \"/homes/dkurtenb/projects/russat/output/spacetrack_tle_df.parquet\"\n",
    "        \"\"\"\n",
    "def print_progress(message):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"{timestamp} - {message}\")\n",
    "\n",
    "\n",
    "# UDL Data to get teh list of NORAD IDs that have TLEs during Feb-April 2022\n",
    "print_progress(\"Reading Parquet file...\")\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "filtered_df = df[(df['year'] == 2022) & ((df['month']==2) | (df['month']==3) | (df['month']==4))]\n",
    "sat_lst = list(filtered_df['satNo'])\n",
    "norad_id_datefilter_lst = list(set(sat_lst))\n",
    "norad_id_datefilter_lst.sort()\n",
    "\n",
    "#Spacetrack Satcat data to get list of norad id that have object type ROCKET BODY, PAYLOAD, or UNKNOWN \n",
    "print_progress(\"Filtering satellites\")\n",
    "with open(sat_cat_pkl, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "norad_list = [item['NORAD_CAT_ID'] for item in data \n",
    "            if item['OBJECT_TYPE'] in ['ROCKET BODY', 'PAYLOAD', 'UNKNOWN']]\n",
    "norad_id_typefilter_lst = list(set(norad_list))\n",
    "norad_id_typefilter_lst.sort()\n",
    "norad_id_typefilter_lst = [int(x) for x in norad_id_typefilter_lst]\n",
    "\n",
    "# Norad IDs filtered by Date (2022 Feb-April) & Type (ROCKET BODY, PAYLOAD, or UNKNOWN)\n",
    "nord_id_typeDate_filter = list(set(norad_id_datefilter_lst) & set(norad_id_typefilter_lst))\n",
    "nord_id_typeDate_filter.sort()\n",
    "print_progress(f\"Found {len(nord_id_typeDate_filter)} satellites after date filtering\")\n",
    "\n",
    "#SpaceTrack Fetch\n",
    "print_progress(\"Starting SpaceTrack fetch\")    \n",
    "with open(login_creds, 'r') as f:\n",
    "    content = f.read()\n",
    "st_un = content.split(\",\")[0].strip()\n",
    "st_pw = content.split(\",\")[1].strip()\n",
    "\n",
    "def fetch_tle_batch(norad_ids, batch_size=50):\n",
    "    all_tles = []\n",
    "    total_batches = (len(norad_ids) + batch_size - 1) // batch_size\n",
    "    failed_batches = []\n",
    "\n",
    "    print_progress(f\"Processing {len(norad_ids)} NORAD IDs in {total_batches} batches\")\n",
    "    \n",
    "    for i in range(0, len(norad_ids), batch_size):\n",
    "        batch = norad_ids[i:i + batch_size]\n",
    "        batch_str = ','.join(str(id) for id in batch)\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        print_progress(f\"Fetching batch {batch_num}/{total_batches} ({len(batch)} IDs)\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 10\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                st = SpaceTrackClient(identity=f'{st_un}', password=f'{st_pw}')\n",
    "                st.timeout = (30,300)\n",
    "\n",
    "                query = st.tle(norad_cat_id=batch_str, orderby='epoch', limit=None, format='tle')\n",
    "                tles = query.split('\\n')\n",
    "                batch_tles = [tles[i:i+2] for i in range(0, len(tles), 2)]\n",
    "                all_tles.extend(batch_tles)\n",
    "                \n",
    "                print_progress(f\"Successfully fetched {len(batch_tles)} TLE pairs\")\n",
    "                time.sleep(15)  \n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print_progress(f\"Error on batch {batch_num}: {str(e)}\")\n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = 300 *(2**retry_count)  \n",
    "                    print_progress(f\"Retrying in {wait_time} seconds... (Attempt {retry_count + 1}/{max_retries})\")                        \n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print_progress(f\"Failed to fetch batch {batch_num} after {max_retries} attempts\")\n",
    "                    failed_batches.append({'batch_num':batch_num,\n",
    "                                            'norad_ids': batch\n",
    "                                            })\n",
    "                    with open('failed_batches.txt','a') as f:\n",
    "                        f.write(f\"Batch {batch_num}:{batch_str}\\n\")\n",
    "\n",
    "    if failed_batches:\n",
    "        print(\"\\nFailed Batches Summary:\")\n",
    "        print(f\"Total failed batches: {len(failed_batches)}\")\n",
    "        print(\"Failed batch numbers:\", [b['batch_num'] for b in failed_batches])  \n",
    "\n",
    "    return all_tles\n",
    "\n",
    "full_lst = fetch_tle_batch(nord_id_typeDate_filter, batch_size=12)    \n",
    "\n",
    "import os\n",
    "file_path = os.path.join(r'C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC', 'spacetrack_tle.txt')\n",
    "with open(file_path, 'w') as f:\n",
    "    for item in full_lst:\n",
    "        f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and parse the Spacetrack Data Sparse - text file is missing recent data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\Russat\\pull\\raw_spacetrack_tle.txt\", 'r') as file:\n",
    "    full_lst = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst):\n",
    "    n = len(lst)\n",
    "    size = n // 4  # Integer division\n",
    "    return [lst[i:i + size] for i in range(0, n, size)]\n",
    "\n",
    "batches = split_list(full_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime\n",
    "\n",
    "#with open(r'C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\spacetrack_tle.txt', 'r') as file:\n",
    "#    full_lst = file.readlines()\n",
    "\n",
    "#length= (len(full_lst))//2\n",
    "#first_half = full_lst[:length]\n",
    "#second_half = full_lst[length:]\n",
    "    \n",
    "#tle_lst = batches[0] \n",
    "#tle_lst = batches[1] \n",
    "#tle_lst = batches[2] \n",
    "#tle_lst = batches[3] \n",
    "tle_lst = batches[4] \n",
    "\n",
    "def print_progress(message):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\")\n",
    "\n",
    "def parse_scientific_notation(string):\n",
    "    try:\n",
    "        string = string.strip()\n",
    "        if string in ['+00000-0', '+00000+0', '']:\n",
    "            return 0.0\n",
    "        mantissa = float(string[0] + '.' + string[1:6])\n",
    "        exponent = int(string[6:8])\n",
    "        return mantissa * (10 ** exponent)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def safe_parse(func, string, default=0):\n",
    "    try:\n",
    "        return func(string)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def parse_tle_strings(tle_strings):\n",
    "    data = []\n",
    "    \n",
    "    for tle_string in tle_strings:\n",
    "        try:\n",
    "            tle = ast.literal_eval(tle_string.strip())\n",
    "            line1, line2 = tle\n",
    "            \n",
    "            if len(line1) < 69 or len(line2) < 69:\n",
    "                continue\n",
    "                \n",
    "            line_data = {\n",
    "                'line1': line1,\n",
    "                'line2': line2,\n",
    "                'catalog_number': safe_parse(int, line1[2:7]),\n",
    "                'classification': line1[7] if len(line1) > 7 else '',\n",
    "                'launch_year': line1[9:11] if len(line1) > 10 else '',\n",
    "                'launch_number': line1[11:14] if len(line1) > 13 else '',\n",
    "                'launch_piece': line1[14:17].strip() if len(line1) > 16 else '',\n",
    "                'epoch_year': safe_parse(int, line1[18:20]),\n",
    "                'epoch_day': safe_parse(float, line1[20:32]),\n",
    "                'mean_motion_dot': safe_parse(float, line1[33:43]),\n",
    "                'mean_motion_ddot': parse_scientific_notation(line1[44:52] + line1[52:54]),\n",
    "                'bstar': parse_scientific_notation(line1[53:61] + line1[61:63]),\n",
    "                'ephemeris_type': safe_parse(int, line1[63:64]),\n",
    "                'element_number': safe_parse(int, line1[64:68]),\n",
    "                'satellite_number': safe_parse(int, line2[2:7]),\n",
    "                'inclination': safe_parse(float, line2[8:16]),\n",
    "                'ra_of_asc_node': safe_parse(float, line2[17:25]),\n",
    "                'eccentricity': safe_parse(float, '0.' + line2[26:33].strip()),\n",
    "                'arg_of_perigee': safe_parse(float, line2[34:42]),\n",
    "                'mean_anomaly': safe_parse(float, line2[43:51]),\n",
    "                'mean_motion': safe_parse(float, line2[52:63]),\n",
    "                'rev_at_epoch': safe_parse(int, line2[63:68])\n",
    "            }\n",
    "            data.append(line_data)\n",
    "        except Exception as e:\n",
    "            print_progress(f\"Error parsing TLE: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not data:  # Check if we have any data before creating DataFrame\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    df['is_valid'] = (df['catalog_number'] > 0) & df['epoch_year'].notna() & df['epoch_day'].notna() & df['mean_motion'].notna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    parquet_out = r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\spacetrack_tle_df_E.parquet\"\n",
    "\n",
    "    \n",
    "    df = parse_tle_strings(tle_lst)\n",
    "    \n",
    "    if not df.empty:\n",
    "        df.to_parquet(parquet_out, engine='pyarrow', compression='gzip', index=True)\n",
    "        print_progress('DONE')\n",
    "    else:\n",
    "        print_progress('No valid TLE data found')\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\dk412\\Desktop\\David\\Python Projects\\RusSat\\dataout_HPC\\spacetrack_tle_df_D.parquet\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satdet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
